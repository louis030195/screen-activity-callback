{
    "frames": [
        {
            "frame_number": 1,
            "text": "LinkedIn Sales Navigator\n\nJohn Doe - CEO at Example Corp\n\nMessage: Hi John, I noticed your company is expanding in the AI sector. I'd love to discuss how our solutions can help you scale efficiently.\n\nProfile: https://www.linkedin.com/in/johndoe/\n\nPROBLEMS 16 OUTPUT DEBUG CONSOLE TERMINAL PORTS\n\nChat\n\n\u00bb R\n\nAa ab, .*\n\n2 of 3 NZ\n\n\"I\nx\n\n> | read_csv\n\nEdit\n\ng Python + v We ~ Xx\n\n[2024-06-02 11:34:03] INFO engine_base.py:198: If you have high concurrent requests and want to maximize the GPU memory utilization, please select mode \"server\".\n\n[11:34:03] /Users/catalyst/Workspace/mlc-ai-package-self-runner/_work/package/package/mlc-1llm/cpp/serve/config.cc:646:\n\nto 8192, prefill chunk size will be set to 1024.\n[11:\nbe set to 8192, prefill chunk size will be set to 1024.\n[11:\nset\n[11:\n192, prefill chunk size is 1024.\n\nto 32768, prefill chunk size will be set to 1024.\n\n[11:34:03] /Users/catalyst/Workspace/mlc-ai-package-self-runner/_work/package/package/mlc-1llm/cpp/serve/config.cc:731:\n\n2.268 MB. Temporary buffer: 335.925 MB). The actual usage might be slightly larger than the estimated number.\n\n0\n\n2 maint OC @o0A8O8 Ov\n\nrust-analyzer\n\n34:03] /Users/catalyst/Workspace/mlc-ai-package-self-runner/_work/package/package/mlc-llm/cpp/serve/config.cc:646:\n34:03] /Users/catalyst/Workspace/mlc-ai-package-self-runner/_work/package/package/mlc-llm/cpp/serve/config.cc:646:\n\n34:03] /Users/catalyst/Workspace/mlc-ai-package-self-runner/_work/package/package/mlc-llm/cpp/serve/config.cc:726:\n\nUnder mode \"local\", max batch size will be set to 4, max KV cache token capacity will be se\nUnder mode \"interactive\", max batch size will be set to 1, max KV cache token capacity will\nUnder mode \"server\", max batch size will be set to 80, max KV cache token capacity will be\nThe actual engine mode is \"local\". So max batch size is 4, max KV cache token capacity is 8\n\nEstimated total single GPU memory usage: 5736.325 MB (Parameters: 4308.133 MB. KVCache: 109\n\nLn 141, Col 21 (8 selected) {\u00a7 Python 3.10.14 (\u2018env': venv)\n\nCopilot++ &\n"
        },
        {
            "frame_number": 2,
            "text": "LinkedIn Sales Navigator\n\nJane Smith - CTO at Tech Innovators\n\nMessage: Hi Jane, I came across your profile and saw your recent post about cloud computing. We have some exciting developments in this area that I think you'll find valuable.\n\nProfile: https://www.linkedin.com/in/janesmith/\n\nPROBLEMS 16 OUTPUT DEBUG CONSOLE TERMINAL PORTS g Python +v Wee aA x\n\n[11:34:03] /Users/catalyst/Workspace/mlc-ai-package-self-runner/_work/package/package/mlc-llm/cpp/serve/config.cc:646: Under mode \"local\", max batch size will be set to 4, max KV cache token capacity will be se\nt to 8192, prefill chunk size will be set to 1024. !\n[11:34:03] /Users/catalyst/Workspace/mlc-ai-package-self-runner/_work/package/package/mlc-llm/cpp/serve/config.cc:646: Under mode \"interactive\", max batch size will be set to 1, max KV cache token capacity will\nbe set to 8192, prefill chunk size will be set to 1024.\n\n[11:34:03] /Users/catalyst/Workspace/mlc-ai-package-self-runner/_work/package/package/mlc-llm/cpp/serve/config.cc:646: Under mode \"server\", max batch size will be set to 80, max KV cache token capacity will be\nset to 32768, prefill chunk size will be set to 1024.\n\n[11:34:03] /Users/catalyst/Workspace/mlc-ai-package-self-runner/_work/package/package/mlc-llm/cpp/serve/config.cc:726: The actual engine mode is \"local\". So max batch size is 4, max KV cache token capacity is 8\n192, prefill chunk size is 1024.\n\n[11:34:03] /Users/catalyst/Workspace/mlc-ai-package-self-runner/_work/package/package/mlc-11m/cpp/serve/config.cc:731: Estimated total single GPU memory usage: 5736.325 MB (Parameters: 4308.133 MB. KVCache: 109\n2.268 MB. Temporary buffer: 335.925 MB). The actual usage might be slightly larger than the estimated number.\n\nCurrent batch size: 1/5\n\n0\n\n& mains O \u00a9\u00aeOA8@8 rust-analyzer OV Ln 141, Col 21 (8 selected) {} Python 3.10.14 (\u2018env': venv) Copilott+ &\n"
        },
        {
            "frame_number": 3,
            "text": "LinkedIn Sales Navigator\n\nMichael Brown - Head of Marketing at Market Leaders\n\nMessage: Hi Michael, I enjoyed reading your article on digital marketing trends. I believe our platform can significantly boost your campaign performance.\n\nProfile: https://www.linkedin.com/in/michaelbrown/\n\nPROBLEMS 16 OUTPUT DEBUG CONSOLE TERMINAL PORTS\nt to 8192, prefill chunk size will be set to 1024.\n\n[11:34:03] /Users/catalyst/Workspace/mlc-ai-package-self-runner/_work/package/package/mlc-1llm/cpp/serve/config.cc:\n\nbe set to 8192, prefill chunk size will be set to 1024.\n\n[11:34:03] /Users/catalyst\n\n[11:34:03] /Users/catalyst/Workspace/mlc-ai-package-self-runner/_work/package/package/mlc-1llm/cpp/serve/config.cc:646:\n\nset to 32768, prefill chunk size will be set to 1024.\n\n[11:34:03] /Users/catalyst/Workspace/mlc-ai-package-self-runner/_work/package/package/mlc-1llm/cpp/serve/config.cc:726:\n\nUnder mode \"local\", max batch size will be set to 4, max KV cache token capacity will be se\nUnder mode \"interactive\", max batch size will be set to 1, max KV cache token capacity will\nUnder mode \"server\", max batch size will be set to 80, max KV cache token capacity will be\nThe actual engine mode is \"local\". So max batch size is 4, max KV cache token capacity is 8\n\nEstimated total single GPU memory usage: 5736.325 MB (Parameters: 4308.133 MB. KVCache: 109\n\nLn 141, Col 21 (8 selected) {\u00a7 Python 3.10.14 (\u2018env': venv)\n\nCopilot++ &\n"
        },
        {
            "frame_number": 4,
            "text": "LinkedIn Sales Navigator\n\nSarah Johnson - VP of Sales at Sales Experts\n\nMessage: Hi Sarah, I saw your recent achievements in sales growth. Our tools can help you maintain and even accelerate this momentum.\n\nProfile: https://www.linkedin.com/in/sarahjohnson/\n\nPROBLEMS 16 OUTPUT DEBUG CONSOLE TERMINAL PORTS g Python +v Wee aA x\n\n[11:34:03] /Users/catalyst/Workspace/mlc-ai-package-self-runner/_work/package/package/mlc-llm/cpp/serve/config.cc:646: Under mode \"local\", max batch size will be set to 4, max KV cache token capacity will be se\nt to 8192, prefill chunk size will be set to 1024. !\n[11:34:03] /Users/catalyst/Workspace/mlc-ai-package-self-runner/_work/package/package/mlc-llm/cpp/serve/config.cc:646: Under mode \"interactive\", max batch size will be set to 1, max KV cache token capacity will\nbe set to 8192, prefill chunk size will be set to 1024.\n\n[11:34:03] /Users/catalyst/Workspace/mlc-ai-package-self-runner/_work/package/package/mlc-llm/cpp/serve/config.cc:646: Under mode \"server\", max batch size will be set to 80, max KV cache token capacity will be\nset to 32768, prefill chunk size will be set to 1024.\n\n[11:34:03] /Users/catalyst/Workspace/mlc-ai-package-self-runner/_work/package/package/mlc-llm/cpp/serve/config.cc:726: The actual engine mode is \"local\". So max batch size is 4, max KV cache token capacity is 8\n192, prefill chunk size is 1024.\n\n[11:34:03] /Users/catalyst/Workspace/mlc-ai-package-self-runner/_work/package/package/mlc-11m/cpp/serve/config.cc:731: Estimated total single GPU memory usage: 5736.325 MB (Parameters: 4308.133 MB. KVCache: 109\n2.268 MB. Temporary buffer: 335.925 MB). The actual usage might be slightly larger than the estimated number.\n\nCurrent batch size: 1/5\n\n0\n\n& mains O \u00a9\u00aeOA8@8 rust-analyzer OV Ln 141, Col 21 (8 selected) {} Python 3.10.14 (\u2018env': venv) Copilott+ &\n"
        },
        {
            "frame_number": 5,
            "text": "LinkedIn Sales Navigator\n\nDavid Lee - Product Manager at Innovative Solutions\n\nMessage: Hi David, I noticed your interest in product management. We have a suite of tools designed to streamline your workflow and enhance productivity.\n\nProfile: https://www.linkedin.com/in/davidlee/\n\nPROBLEMS 16 OUTPUT DEBUG CONSOLE TERMINAL PORTS g Python +v Wee aA x\n\n[11:34:03] /Users/catalyst/Workspace/mlc-ai-package-self-runner/_work/package/package/mlc-llm/cpp/serve/config.cc:646: Under mode \"local\", max batch size will be set to 4, max KV cache token capacity will be se\nt to 8192, prefill chunk size will be set to 1024. !\n[11:34:03] /Users/catalyst/Workspace/mlc-ai-package-self-runner/_work/package/package/mlc-llm/cpp/serve/config.cc:646: Under mode \"interactive\", max batch size will be set to 1, max KV cache token capacity will\nbe set to 8192, prefill chunk size will be set to 1024.\n\n[11:34:03] /Users/catalyst/Workspace/mlc-ai-package-self-runner/_work/package/package/mlc-llm/cpp/serve/config.cc:646: Under mode \"server\", max batch size will be set to 80, max KV cache token capacity will be\nset to 32768, prefill chunk size will be set to 1024.\n\n[11:34:03] /Users/catalyst/Workspace/mlc-ai-package-self-runner/_work/package/package/mlc-llm/cpp/serve/config.cc:726: The actual engine mode is \"local\". So max batch size is 4, max KV cache token capacity is 8\n192, prefill chunk size is 1024.\n\n[11:34:03] /Users/catalyst/Workspace/mlc-ai-package-self-runner/_work/package/package/mlc-11lm/cpp/serve/config.cc:731: Estimated total single GPU memory usage: 5736.325 MB (Parameters: 4308.133 MB. KVCache: 109\n2.268 MB. Temporary buffer: 335.925 MB). The actual usage might be slightly larger than the estimated number.\n\nCurrent batch size: 1/5\n\nCurrent batch size: 2/5\n\nCurrent batch size: 3/5\n\nCurrent batch size: 4/5\n\n0\n\n& mains O \u00a9\u00aeOA8@8 rust-analyzer OV Ln 141, Col 21 (8 selected) {} Python 3.10.14 (\u2018env': venv) Copilott+ &\n"
        }
    ]
}